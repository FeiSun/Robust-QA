<!--
 * @Author: leiyan leiyan21@mails.ucas.ac.cn
 * @Date: 2022-08-25 11:14:23
 * @LastEditors: leiyan leiyan21@mails.ucas.ac.cn
 * @LastEditTime: 2022-09-08 10:17:55
 * @FilePath: /undefined/Users/leiyan/Paper/Robust-QA/README.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
# Robust-QA
Robust QA: attack, defense, robust

**QA attack at inference stage**

[Adversarial Examples for Evaluating Reading Comprehension Systems](https://aclanthology.org/D17-1215.pdf)

[Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering](https://arxiv.org/pdf/2112.09658.pdf)

[T3: Tree-Autoencoder Regularized Adversarial Text Generation for Targeted Attack](https://aclanthology.org/2020.emnlp-main.495.pdf)


**VQA attack at training stage**

[Dual-Key Multimodal Backdoors for Visual Question Answering](https://openaccess.thecvf.com/content/CVPR2022/papers/Walmer_Dual-Key_Multimodal_Backdoors_for_Visual_Question_Answering_CVPR_2022_paper.pdf)


**NLP attack at training stage**

[BadNL: Backdoor Attacks Against NLP Models](https://openreview.net/pdf?id=v6UimxiiR78)

[Rethinking Stealthiness of Backdoor Attack against NLP Models](https://aclanthology.org/2021.acl-long.431.pdf)

[Concealed Data Poisoning Attacks on NLP Models](https://arxiv.org/pdf/2010.12563.pdf)

[Weight Poisoning Attacks on Pre-trained Models](https://arxiv.org/pdf/2004.06660.pdf)


**Defense agnist NLP backdoo**

[ONION: A Simple and Effective Defense Against Textual Backdoor Attacks](https://aclanthology.org/2021.emnlp-main.752.pdf)

**CSCI 699 course**

[THIEVES ON SESAME STREET! MODEL EXTRACTION OF BERT-BASED APIS](https://arxiv.org/pdf/1910.12366.pdf) [model steadling]

[Imitation Attacks and Defenses for Black-box Machine Translation Systems](https://arxiv.org/pdf/2004.15015.pdf) [model steadling]

**ACL backdoor in NLP**

[Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution](https://arxiv.org/pdf/2106.06361.pdf)

[Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger](https://arxiv.org/pdf/2105.12400.pdf)

**EMNLP backdoor in NLP**

[Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning](https://arxiv.org/pdf/2108.13888.pdf)

[RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models](https://aclanthology.org/2021.emnlp-main.659.pdf)

[Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer](https://arxiv.org/pdf/2110.07139.pdf)

[ONION: A Simple and Effective Defense Against Textual Backdoor Attacks](https://arxiv.org/pdf/2011.10369.pdf)

**NAACL backdoor in NLP**

[Triggerless Backdoor Attack for NLP Tasks with Clean Label](https://arxiv.org/pdf/2111.07970.pdf)

**AAAI backdoor in NLP**

[Hard to Forget: Poisoning Attacks on Certified Machine Unlearning](https://arxiv.org/pdf/2109.08266.pdf)

[Backdoor Attacks on the DNN Interpretation System](https://arxiv.org/pdf/2011.10698.pdf)

[Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks](https://arxiv.org/pdf/2008.04495.pdf)

[DeHiB: Deep Hidden Backdoor Attack on Semi-supervised Learning via Adversarial Perturbation](https://ojs.aaai.org/index.php/AAAI/article/view/17266)

[Hidden Trigger Backdoor Attacks](https://arxiv.org/pdf/1910.00033.pdf)

**ICLR backdoor in NLP**

[POISONING AND BACKDOORING CONTRASTIVE LEARNING](https://openreview.net/pdf?id=iC4UHbQ01Mp) by Google

[HOW TO INJECT BACKDOORS WITH BETTER CONSISTENCY: LOGIT ANCHORING ON CLEAN DATA](https://openreview.net/pdf?id=Bn09TnDngN)

[TRIGGER HUNTING WITH A TOPOLOGICAL PRIOR FOR TROJAN DETECTION](https://openreview.net/pdf?id=TXsjU8BaibT)


**Useful Repos**
[OpenBackdoor](https://github.com/thunlp/OpenBackdoor)


**Backdoor on generative model**
[Adversarial Attacks Against Deep Generative Models on Data: A Survey](https://arxiv.org/pdf/2112.00247.pdf)
[Poisoning Attack on Deep Generative Models in Autonomous Driving](https://www.cs.wm.edu/~liqun/paper/securecomm19.pdf)

